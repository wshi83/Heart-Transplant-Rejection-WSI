{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSI histogram to label using Tree-based ML models  (STEP 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = os.sep\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment=True\n",
    "augmentation_type = 'diffusion' #can also be 'diffusion'\n",
    "model_type = 'densenet161' #can also be 'vgg19' or 'resnet50' or 'resnet152' or 'densenet161'\n",
    "iteration = 1\n",
    "random_state = 1234\n",
    "\n",
    "\n",
    "## Augmented ##\n",
    "if(augment): \n",
    "    # Rejection Scores Filename\n",
    "    rejection_score_filename = f\"rejection_scores_{model_type}_{timeStamp}_augmented_{augmentation_type}_{dataset_type}_iteration_{iteration}_seed_{random_state}.npy\"\n",
    "    ### Figure: ROC ###\n",
    "    figure_ROC_path = f\"{current_dir}{sep}figures{sep}Biopsy_ROC_{model_type}_augmented_{augmentation_type}_{dataset_type}_iteration_{iteration}_seed_{random_state}.png\"\n",
    "    ### Figure: Confusion Matrix ###\n",
    "    figure_CM_path = f\"{current_dir}{sep}figures{sep}ConfusionMatrix_{model_type}_augmented_{augmentation_type}_{dataset_type}_iteration_{iteration}_seed_{random_state}.png\"\n",
    "## Original ##\n",
    "else: \n",
    "    # Output Name\n",
    "    rejection_score_filename = f\"rejection_scores_{model_type}_{timeStamp}_{dataset_type}_iteration_{iteration}_seed_{random_state}.npy\"\n",
    "    ### Figure: ROC ###\n",
    "    figure_ROC_path = f\"{current_dir}{sep}figures{sep}Biopsy_ROC_{model_type}_{dataset_type}_iteration_{iteration}_seed_{random_state}.png\"\n",
    "    ### Figure: Confusion Matrix ###\n",
    "    figure_CM_path = f\"{current_dir}{sep}figures{sep}ConfusionMatrix_{model_type}_{dataset_type}_iteration_{iteration}_seed_{random_state}.png\"\n",
    "    \n",
    "## Bin Hyperparameter ##\n",
    "num_bins = [2,10,50,100,200,300,400]\n",
    "\n",
    "\n",
    "## Metadata ##\n",
    "metadata_path = f\"{sep}home{sep}mainuser{sep}fast_datadrive{sep}HeartTransplantData{sep}Metadata{sep}Metadata_wsi_previous.csv\"\n",
    "metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "## WSI Dataset Path ##\n",
    "wsi_train_path = f\"{sep}home{sep}mainuser{sep}fast_datadrive{sep}HeartTransplantData{sep}2021_Data{sep}wsi_classifier_training_val_data{sep}wsi_train{sep}\"\n",
    "wsi_test_path = f\"{sep}home{sep}mainuser{sep}fast_datadrive{sep}HeartTransplantData{sep}2021_Data{sep}wsi_classifier_training_val_data{sep}wsi_test{sep}\"\n",
    "\n",
    "## Training Data ##\n",
    "train_wsis = glob.glob(wsi_train_path +'[SC]*[0-9]')\n",
    "\n",
    "## Test Data ##\n",
    "test_wsis = glob.glob(wsi_test_path +'[SC]*[0-9]')\n",
    "\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate Performance Metrics ###\n",
    "def generate_auroc_mcc(predicted, actual, title, show_output=False, fig_save_path=False):  \n",
    "    ### Confusion Matrix Values ###\n",
    "    tps,fps,fns,tns = calc_conf_matrix((predicted > 0.5).astype(int), actual.astype(int))\n",
    "    \n",
    "    ## AUROC ##\n",
    "    auc_score = metrics.roc_auc_score(y_true=actual, y_score=predicted);\n",
    "    ### FPR and TPR ###\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_true=actual, y_score=predicted, pos_label=1);\n",
    "    ## MCC ##\n",
    "    if(tps == 0 or tns == 0):\n",
    "        mcc_score = 0\n",
    "    else:\n",
    "        mcc_score = metrics.matthews_corrcoef(y_true=actual, y_pred=(predicted > 0.5).astype(int) );\n",
    "    \n",
    "    ### Plot ###\n",
    "    if show_output:\n",
    "        plt.plot(fpr, tpr, color=\"darkorange\", label=\"ROC curve (AUC = %0.4f, MCC = %0.4f)\" % (auc_score, mcc_score));\n",
    "        plt.plot([0, 1], [0, 1], color=\"navy\");\n",
    "        plt.xlim([0.0, 1.0]);\n",
    "        plt.ylim([0.0, 1.05]);\n",
    "        plt.title(title, fontsize = 20);\n",
    "        plt.xlabel(\"False Positive Rate (FPR)\", fontsize = 16);\n",
    "        plt.ylabel(\"True Positive Rate (TPR)\", fontsize = 16);\n",
    "        plt.legend(loc=\"lower right\", fontsize = 14);\n",
    "        \n",
    "        ### Save ###\n",
    "        if(fig_save_path):\n",
    "            plt.savefig(fig_save_path, bbox_inches=\"tight\");\n",
    "        plt.show();\n",
    "        plt.close();\n",
    "        plt.clf();\n",
    "    return auc_score, mcc_score\n",
    "\n",
    "### Generate Confusion Matrix Values ###\n",
    "def calc_conf_matrix(predicted, actual, verbose=False):\n",
    "    if(verbose):\n",
    "        print('predicted: %s'% predicted)\n",
    "        print('actual: %s'% actual)\n",
    "    tps = np.sum(np.logical_and(predicted == 1, actual == 1))\n",
    "    fps = np.sum(np.logical_and(predicted == 1, actual == 0))\n",
    "    fns = np.sum(np.logical_and(predicted == 0, actual == 1))\n",
    "    tns = np.sum(np.logical_and(predicted == 0, actual == 0))\n",
    "    return tps, fps, fns, tns\n",
    "\n",
    "### Generate WSI Performance Metrics ###\n",
    "def calc_wsi_metrics(predicted, actual, title, show_output=False, fig_save_path=False):\n",
    "    ### Confusion Matrix Values ###\n",
    "    tps,fps,fns,tns = calc_conf_matrix((predicted > 0.5).astype(int), actual.astype(int))\n",
    "    # print(\"TP: %f, FP: %f, FN: %f, TN: %f\" % (tps, fps, fns, tns))\n",
    "    \n",
    "    ### Accuracy ###\n",
    "    total = actual.shape[0]\n",
    "    acc = (tps + tns)/total\n",
    "    \n",
    "    ### Sensitivity ###\n",
    "    sens  = np.NaN\n",
    "    if(tps + fns > 0):\n",
    "        sens = tps / (tps + fns)\n",
    "    ### Specificity ###\n",
    "    spec = np.NaN\n",
    "    if(tns + fps > 0):\n",
    "        spec = tns / (tns + fps) \n",
    "        \n",
    "    ### Confusion Matrix ###\n",
    "    conf_matrix = np.array([[tps, fps], [fns, tns]])\n",
    "    auc_score, mcc_score = generate_auroc_mcc(predicted=predicted, actual=actual, \n",
    "                                              title=title, show_output=show_output, fig_save_path=fig_save_path);\n",
    "    \n",
    "    ### Return ###\n",
    "    return acc, sens, spec, auc_score, mcc_score, conf_matrix\n",
    "\n",
    "### Plot Confusion Matrix ###\n",
    "def plot_confusion_matrix(confusion_matrix, title, save_fig_path=False):\n",
    "    conf = confusion_matrix / confusion_matrix.sum(axis=1, keepdims=True)\n",
    "    confusion_plot = sns.heatmap(conf,annot=confusion_matrix,cmap='Blues',fmt=\".0f\", xticklabels=[\"Rejection\", \"Nonrejection\"],\n",
    "    yticklabels=[\"Rejection\", \"Nonrejection\"],cbar=False, robust=True)\n",
    "    confusion_plot.set_xlabel(\"Actual\",fontsize=15)\n",
    "    confusion_plot.set_ylabel(\"Predicted\",fontsize=15)\n",
    "    confusion_plot.set_title(title)\n",
    "    \n",
    "    if(save_fig_path):\n",
    "        if(not os.path.exists(path)):\n",
    "            os.makedirs(path)\n",
    "        confusion_plot.get_figure().savefig(save_fig_path)\n",
    "    \n",
    "    plt.clf()\n",
    "\n",
    "def calc_histogram(hist_arr, wsi_name='', bins=10, show=False):\n",
    "    ### Bins ###\n",
    "    hist_bins = np.linspace( 0, 1.0, bins+1 )\n",
    "    ### Histogram ###\n",
    "    histogram_wsi = np.histogram(hist_arr, bins=hist_bins, density=True)\n",
    "    ### Plot ###\n",
    "    if show:\n",
    "        plt.hist(hist_arr, bins=hist_bins, density=True)\n",
    "        plt.xticks(np.arange(0,1,0.1))\n",
    "        plt.title(wsi_name)\n",
    "        plt.xlabel(\"Probability\")\n",
    "        plt.ylabel(\"Count\")\n",
    "    return histogram_wsi\n",
    "\n",
    "### Return Histogram Bin values ###\n",
    "def gen_data(bin_idx, wsi_paths, rejection_score_filename):\n",
    "    input_arr = []\n",
    "    labels = []\n",
    "    for wsi_path in wsi_paths:\n",
    "        wsi_name = wsi_path.split(\"/\")[-1]\n",
    "        # print(wsi_name)\n",
    "        hist_arr = np.load(wsi_path + \"/\" + rejection_score_filename)\n",
    "        \n",
    "        ### Histogram with bin_idx bins ###\n",
    "        hist_details = calc_histogram(hist_arr, wsi_name, bins=bin_idx, show=False)\n",
    "        input_arr.append(hist_details[0])\n",
    "        \n",
    "        ### Label ###\n",
    "        label = metadata_df[metadata_df[\"Filename\"] == f\"{wsi_name}\"][\"Label\"].iloc[0]\n",
    "        labels.append(label)\n",
    "    return input_arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hyperparameter Tuning (5-Fold CV is the default for GridSearchCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"XGBoost_RF\"\n",
    "for loop_idx,bin_idx in enumerate(num_bins):\n",
    "    print('%s / %s'%(loop_idx,len(num_bins)), end='\\r')\n",
    "    train_arr, train_labels = gen_data(bin_idx, wsi_paths=train_wsis, rejection_score_filename=rejection_score_filename)\n",
    "    model = xgb.XGBRFClassifier(random_state=random_seed)\n",
    "    model = GridSearchCV(model, {'max_depth': [2,3,4,5,6,7,8,9,10],\n",
    "                                 'n_estimators': [2,3,4,5,6,10,25,50, 100, 200, 300, 400], \n",
    "                                 'learning_rate':[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4]},\n",
    "                         n_jobs=-1)\n",
    "    model.fit(train_arr, train_labels)\n",
    "    if(not model_name in models.keys()):\n",
    "        models[model_name] = {}\n",
    "    models[model_name][bin_idx] = {'model':model, 'score':model.best_score_}\n",
    "print('DONE     ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"XGBoost\"\n",
    "for loop_idx,bin_idx in enumerate(num_bins):\n",
    "    print('%s / %s'%(loop_idx,len(num_bins)), end='\\r')\n",
    "    train_arr, train_labels = gen_data(bin_idx, wsi_paths=train_wsis, rejection_score_filename=rejection_score_filename)\n",
    "    model = xgb.XGBClassifier(random_state=random_seed)\n",
    "    model = GridSearchCV(model, {'max_depth': [2,3,4,5,6,7,8,9,10],\n",
    "                                 'n_estimators': [2,3,4,5,6,10,25,50, 100, 125, 150, 175, 200, 300, 400], \n",
    "                                 'learning_rate':[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4]},\n",
    "                         n_jobs=-1)\n",
    "    model.fit(train_arr, train_labels)\n",
    "    if(not model_name in models.keys()):\n",
    "        models[model_name] = {}\n",
    "    models[model_name][bin_idx] = {'model':model, 'score':model.best_score_}\n",
    "print('DONE     ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Random Forest Classifier\"\n",
    "for loop_idx,bin_idx in enumerate(num_bins):\n",
    "    print('%s / %s'%(loop_idx,len(num_bins)), end='\\r')\n",
    "    train_arr, train_labels = gen_data(bin_idx, wsi_paths=train_wsis, rejection_score_filename=rejection_score_filename)\n",
    "    model = RandomForestClassifier(random_state=random_seed)\n",
    "    model = GridSearchCV(model, {'max_depth': [2,3,4,5,6,7,8,9,10], \n",
    "                                 'n_estimators': [2,3,4,5,6,10,25,50, 100, 125, 150, 175, 200, 300, 400]},\n",
    "                         n_jobs=-1)\n",
    "    model.fit(train_arr, train_labels)\n",
    "    if(not model_name in models.keys()):\n",
    "        models[model_name] = {}\n",
    "    models[model_name][bin_idx] = {'model':model, 'score':model.best_score_}\n",
    "print('DONE     ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"Decision Tree Classifier\"\n",
    "for loop_idx,bin_idx in enumerate(num_bins):\n",
    "    print('%s / %s'%(loop_idx,len(num_bins)), end='\\r')\n",
    "    train_arr, train_labels = gen_data(bin_idx, wsi_paths=train_wsis, rejection_score_filename=rejection_score_filename)\n",
    "    model = sklearn.tree.DecisionTreeClassifier(random_state=random_seed)\n",
    "    model = GridSearchCV(model, {'min_samples_split': list( range(1,20) ),\n",
    "                                 \"min_samples_leaf\": list( range(1,20) )},\n",
    "                         n_jobs=-1)\n",
    "    model.fit(train_arr, train_labels)\n",
    "    if(not model_name in models.keys() ):\n",
    "        models[model_name] = {}\n",
    "    models[model_name][bin_idx] = {'model':model, 'score':model.best_score_}\n",
    "print('DONE     ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance: Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Iterate across Bins ###\n",
    "outcome_df={}\n",
    "for bin_idx in num_bins:\n",
    "    ### Generate Test Dataset Features and Labels ###\n",
    "    test_arr, test_labels = gen_data(bin_idx, wsi_paths=test_wsis, rejection_score_filename=rejection_score_filename)\n",
    "    \n",
    "    ### Iterate across Models ###\n",
    "    for model_name, model_info in models.items():\n",
    "        ### Model ###\n",
    "        model = model_info[bin_idx]['model'] \n",
    "        \n",
    "        ### Prediction ###\n",
    "        probs = model.predict_proba(test_arr)[:,1]\n",
    "        \n",
    "        ### Performance ###\n",
    "        acc, sens, spec, auc_score, mcc_score,conf_matrix = calc_wsi_metrics(predicted=np.array(probs),\n",
    "                                                                             actual=np.array(test_labels),\n",
    "                                                                             title='%s Bins: %s'%(model_name,bin_idx),\n",
    "                                                                             show_output=False)\n",
    "\n",
    "        ### Performance Dictionary ###\n",
    "        outcome_dict = {\n",
    "            'Model':[model_name],\n",
    "            'Score (Validation)': [model_info[bin_idx]['score']],\n",
    "            'Score (Test)': [model.score(test_arr,test_labels)],\n",
    "            'Accuracy (Test)':[acc],\n",
    "            'AUROC (Test)':[auc_score],\n",
    "            'MCC (Test)':[mcc_score],\n",
    "            'Sensitivity (Test)':[sens],\n",
    "            'Specificity (Test)': [spec],\n",
    "            'Bins': [bin_idx] \n",
    "        }\n",
    "        ### Append Model Performance to outcome_df as Row ###\n",
    "        if( len(outcome_df) == 0):\n",
    "            outcome_df = pd.DataFrame(data=outcome_dict )\n",
    "        else:\n",
    "            outcome_df = pd.concat([outcome_df, pd.DataFrame(data=outcome_dict)], ignore_index = True)\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sort by MCC ###\n",
    "outcome_df = outcome_df.sort_values(by=['MCC (Test)'], ascending=False )\n",
    "pathlib.Path(f'{current_dir}{sep}results{sep}WSI_Classification{sep}WSI_results_{model_type}_{dataset_type}{sep}').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "outcome_file_path = f\"{current_dir}{sep}results{sep}WSI_Classification{sep}\"\\\n",
    "+ f\"WSI_results_{model_type}_{dataset_type}{sep}\"\\\n",
    "+ f\"WSI_results_{model_type}_{dataset_type}_state_{random_state}_iteration_{iteration}.csv\"\n",
    "\n",
    "outcome_df.to_csv(outcome_file_path)\n",
    "outcome_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "x = ast.literal_eval('[1, 2 , 3, 4]')\n",
    "for i in range(len(x)):\n",
    "    print(x[i])\n",
    "    print(type(x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Best Model ###\n",
    "bin_idx= outcome_df.iloc[0][\"Bins\"]\n",
    "# model_name = outcome_df.iloc[0][\"Model\"]\n",
    "model_name = 'Random Forest Classifier'\n",
    "### Model ###\n",
    "model = models[model_name][bin_idx]['model']\n",
    "\n",
    "### Generate Test Dataset Features and Labels ###\n",
    "test_arr, test_labels = gen_data(bin_idx, wsi_paths=test_wsis, rejection_score_filename=rejection_score_filename)\n",
    "\n",
    "### Predictions ###\n",
    "probs = model.predict_proba(test_arr)[:,1]\n",
    "\n",
    "### Performance and Plot ROC ###\n",
    "if(augment):\n",
    "    title = f\"{model_name} with {model_type} classifier and {augmentation_type} augmentation\"\n",
    "else:\n",
    "    title = f\"{model_name} with {model_type} classifier original\"\n",
    "acc, sens, spec, auc_score, mcc_score, conf_matrix = calc_wsi_metrics(np.array(probs),\n",
    "                                                                   np.array(test_labels),\n",
    "                                                                   title=title,\n",
    "                                                                   show_output=True, fig_save_path=figure_ROC_path)\n",
    "### Normalized Confusion Matrix ###\n",
    "if( np.any(conf_matrix.sum(axis=1, keepdims=True), where=0) ):\n",
    "    conf_intensity = conf_matrix / conf_matrix.sum(axis=1, keepdims=True)\n",
    "else:\n",
    "    conf_intensity = conf_matrix\n",
    "\n",
    "### Plot: Confusion Matrix ###\n",
    "if(augment):\n",
    "    s = sns.heatmap(conf_intensity, annot=conf_matrix, cmap='Blues',fmt=\".0f\", xticklabels=[\"Rejection\", \"Nonrejection\"],\n",
    "                    yticklabels=[\"Rejection\", \"Nonrejection\"], cbar=False, robust=True, annot_kws={\"size\": 32})\n",
    "else: \n",
    "    s = sns.heatmap(conf_intensity, annot=conf_matrix, cmap='Oranges',fmt=\".0f\", xticklabels=[\"Rejection\", \"Nonrejection\"],\n",
    "                    yticklabels=[\"Rejection\", \"Nonrejection\"], cbar=False, robust=True, annot_kws={\"size\": 32})        \n",
    "\n",
    "## Axis Labels ##\n",
    "s.set_xlabel(\"Actual\",fontsize=21)\n",
    "s.set_xticklabels(labels=s.get_xticklabels(), va='center', fontsize = 14)\n",
    "s.set_ylabel(\"Predicted\",fontsize=21)\n",
    "s.set_yticklabels(labels=s.get_yticklabels(), va='center', fontsize = 14)\n",
    "\n",
    "if(augment):\n",
    "    s.set_title(f\"{model_name} with {model_type} classifier and {augmentation_type} augmentation\",fontsize=18)\n",
    "else:\n",
    "    s.set_title(f\"{model_name} with {model_type} classifier original\",fontsize=18)\n",
    "plt.savefig(figure_CM_path, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "plt.close();\n",
    "plt.clf();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d5af6d5bcba7d1fff531a29281c149aa828074fe5b928d6fb939f9b784debaa"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
